{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1tiP0FKqlZHu3YcKGY435RxgJ6ScDE1Ev",
      "authorship_tag": "ABX9TyOwAPdsLbIqAIscymcMI+B3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olwynodpatterson/FYP/blob/main/10kForms_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "SNoAMHMgyLLx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e62342be-96da-4393-98d0-557e1de2a710"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scraping 10k fillings"
      ],
      "metadata": {
        "id": "0ALLdZ_9aNvH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "# Function to retrieve a list of tickers from a given URL\n",
        "def get_ticker_list(url):\n",
        "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=headers)\n",
        "    # Check if the request was successful\n",
        "    if response.status_code == 200:\n",
        "        tickers = response.text.splitlines()\n",
        "        return tickers\n",
        "    else:\n",
        "        # Raise an exception if the request failed\n",
        "        raise Exception(f\"Failed to retrieve data. Status code: {response.status_code}\")\n",
        "\n",
        "# Function to choose a random CIK (Central Index Key) from the list of tickers\n",
        "def choose_random_cik(tickers):\n",
        "    # Choose a random ticker and associated CIK\n",
        "    ticker, cik = random.choice(tickers).split('\\t')\n",
        "    return cik\n",
        "\n",
        "# Function to get the URLs of 10-K filings for a given CIK\n",
        "def get_10k_filing_urls(cik):\n",
        "    time.sleep(3) # Delay to avoid overwhelming the server\n",
        "    # Fetch the URLs for 10-K filings\n",
        "    filings_url = f\"https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK={cik}&type=10-K&dateb=&owner=exclude&count=10\"\n",
        "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(filings_url, headers=headers)\n",
        "    # Check response status\n",
        "    if response.status_code != 200:\n",
        "        print(\"Error fetching filings:\", response.status_code)\n",
        "        return None\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    table = soup.find('table', class_='tableFile2')\n",
        "    if not table:\n",
        "        return None\n",
        "\n",
        "    # Parse the table to get the filing links\n",
        "    for row in table.find_all('tr')[1:]:\n",
        "        cols = row.find_all('td')\n",
        "        if len(cols) > 1:\n",
        "            documents_page_link = 'https://www.sec.gov' + cols[1].a['href']\n",
        "            submission_text_file_link = documents_page_link.replace('-index.htm', '.txt')\n",
        "            return submission_text_file_link  # Return the first link found\n",
        "\n",
        "    return None\n",
        "\n",
        "def dehtml(html_content):\n",
        "    # Use BeautifulSoup to parse the HTML content\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "    # Extract text from the parsed HTML\n",
        "    text = soup.get_text()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def download_file(url, cik, folder='/content/drive/My Drive/10k_filings'):\n",
        "    # Define the Google Drive folder path for the specific CIK\n",
        "    drive_folder_path = os.path.join(folder, cik)\n",
        "\n",
        "    # Create the folder if it doesn't exist\n",
        "    if not os.path.exists(drive_folder_path):\n",
        "        os.makedirs(drive_folder_path)\n",
        "\n",
        "    # Correct the file paths to include the CIK-specific folder\n",
        "    html_file_path = os.path.join(drive_folder_path, f'{cik}_10k_form.html')\n",
        "    text_file_path = os.path.join(drive_folder_path, f'{cik}_10k_form.txt')\n",
        "\n",
        "    # Make a request to download the file\n",
        "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=headers, stream=True)\n",
        "    if response.status_code == 200:\n",
        "        with open(html_file_path, 'wb') as html_file:\n",
        "            for chunk in response.iter_content(chunk_size=8192):\n",
        "                html_file.write(chunk)\n",
        "        print(f\"Downloaded HTML: {html_file_path}\")\n",
        "\n",
        "        # Read the HTML content\n",
        "        with open(html_file_path, 'r', encoding='utf-8') as html_file:\n",
        "            html_content = html_file.read()\n",
        "\n",
        "        # Convert HTML to plain text\n",
        "        plain_text = dehtml(html_content)\n",
        "\n",
        "        # Save the plain text\n",
        "        with open(text_file_path, 'w', encoding='utf-8') as text_file:\n",
        "            text_file.write(plain_text)\n",
        "        print(f\"Converted to Text: {text_file_path}\")\n",
        "    else:\n",
        "        print(f\"Failed to download file. Status code: {response.status_code}\")\n",
        "\n",
        "\n",
        "# Main function to drive the program\n",
        "def main():\n",
        "    url = 'https://www.sec.gov/include/ticker.txt'\n",
        "    tickers = get_ticker_list(url)\n",
        "    i=0\n",
        "    while i <2:\n",
        "        cik =  choose_random_cik(tickers)\n",
        "        print(f\"Selected CIK: {cik}\")\n",
        "        filing_url = get_10k_filing_urls(cik)\n",
        "        print(f\"Filing URL: {filing_url}\")\n",
        "        if filing_url:\n",
        "            download_file(filing_url, cik)\n",
        "            time.sleep(1)  # Respectful delay between requests\n",
        "            i += 1\n",
        "        else:\n",
        "            print(f\"No 10-K filings found for CIK {cik}.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXp4d8ZyppGd",
        "outputId": "0b0f9a15-c51f-45a2-b4c7-f744490432f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Selected CIK: 1818089\n",
            "Filing URL: https://www.sec.gov/Archives/edgar/data/1818089/000110465923081092/0001104659-23-081092.txt\n",
            "Downloaded HTML: /content/drive/My Drive/10k_filings/1818089/1818089_10k_form.html\n",
            "Converted to Text: /content/drive/My Drive/10k_filings/1818089/1818089_10k_form.txt\n",
            "Selected CIK: 711665\n",
            "Filing URL: https://www.sec.gov/Archives/edgar/data/711665/000161577419005072/0001615774-19-005072.txt\n",
            "Downloaded HTML: /content/drive/My Drive/10k_filings/711665/711665_10k_form.html\n",
            "Converted to Text: /content/drive/My Drive/10k_filings/711665/711665_10k_form.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finding the reports and converting to PDF\n"
      ],
      "metadata": {
        "id": "m4hd7jRhDhzv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "import pdfkit  # Ensure pdfkit is installed\n",
        "\n",
        "# Function to clean up HTML content\n",
        "def clean_html(html_content):\n",
        "    # Replace non-breaking spaces with regular spaces\n",
        "    cleaned_content = html_content.replace(u'\\xa0', u' ')\n",
        "    return cleaned_content\n",
        "\n",
        "# Directory containing your 10-K filings\n",
        "base_directory_path = '/content/drive/My Drive/10k_filings'\n",
        "\n",
        "# List of reports to search for\n",
        "reports = [\n",
        "    'Balance Sheet',\n",
        "    'Cash Flows',\n",
        "    'Statements of Operations',\n",
        "    'Statement of Operations',\n",
        "    'Statements of Changes in Stockholder’s Equity'\n",
        "]\n",
        "\n",
        "# Loop through all subdirectories in the base directory\n",
        "for subdir in os.listdir(base_directory_path):\n",
        "    subdir_path = os.path.join(base_directory_path, subdir)\n",
        "\n",
        "    # Check if the path is a directory to avoid trying to open files directly\n",
        "    if os.path.isdir(subdir_path):\n",
        "        # Loop through all files in the subdirectory\n",
        "        for filename in os.listdir(subdir_path):\n",
        "            # Check if the file is an HTML file\n",
        "            if filename.endswith('.html'):\n",
        "                print(f\"Processing {filename}\")\n",
        "                file_path = os.path.join(subdir_path, filename)\n",
        "\n",
        "                # Open and read the file\n",
        "                with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                    html_content = file.read()\n",
        "\n",
        "                # Clean the HTML content\n",
        "                cleaned_html = clean_html(html_content)\n",
        "\n",
        "                # Parse the cleaned HTML content\n",
        "                soup = BeautifulSoup(cleaned_html, 'html.parser')\n",
        "\n",
        "                for report_name in reports:\n",
        "                    # Find the <a> tag that contains the current report name\n",
        "                    a_tag = soup.find('a', string=lambda text: text and report_name in text)\n",
        "                    if a_tag and a_tag.has_attr('href'):\n",
        "                        href = a_tag['href']\n",
        "\n",
        "                        # Check if it's an internal link\n",
        "                        if href.startswith('#'):\n",
        "                            target_id = href[1:]  # Remove the '#' at the beginning\n",
        "                            target_element = soup.find(id=target_id)\n",
        "\n",
        "                            if target_element:\n",
        "                                # Find the next table after the target element\n",
        "                                next_table = target_element.find_next('table')\n",
        "                                if next_table:\n",
        "                                    # Convert just the table or the section containing the table to a string\n",
        "                                    target_html_string = str(next_table)\n",
        "\n",
        "                                    # Generate a dynamic PDF file path based on the HTML file name and report name\n",
        "                                    pdf_file_name = f\"{filename.replace('.html', '')}_{report_name.replace(' ', '_').lower()}.pdf\"\n",
        "                                    pdf_file_path = os.path.join(subdir_path, pdf_file_name)\n",
        "\n",
        "                                    # Specify PDF options, including UTF-8 encoding\n",
        "                                    options = {\n",
        "                                        'encoding': \"UTF-8\"\n",
        "                                    }\n",
        "\n",
        "                                    # Convert the targeted HTML content to PDF with specified options\n",
        "                                    pdfkit.from_string(target_html_string, pdf_file_path, options=options)\n",
        "                                    print(f\"{report_name} table saved to {pdf_file_path}\")\n",
        "\n",
        "                                else:\n",
        "                                    print(f\"No next table found for {report_name} in {filename}.\")\n",
        "\n",
        "                            else:\n",
        "                                print(f\"Target element not found for {report_name} in {filename}.\")\n",
        "                        else:\n",
        "                            print(f\"The link in {filename} for {report_name} is not an internal link.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vmqs_WdeBA7R",
        "outputId": "eacb1586-ed16-4031-ec89-ac6ccdf9830f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 1005229_10k_form.html\n",
            "Balance Sheet table saved to /content/drive/My Drive/10k_filings/1005229/1005229_10k_form_balance_sheet.pdf\n",
            "Cash Flows table saved to /content/drive/My Drive/10k_filings/1005229/1005229_10k_form_cash_flows.pdf\n",
            "Statements of Operations table saved to /content/drive/My Drive/10k_filings/1005229/1005229_10k_form_statements_of_operations.pdf\n",
            "Processing 1850906_10k_form.html\n",
            "Balance Sheet table saved to /content/drive/My Drive/10k_filings/1850906/1850906_10k_form_balance_sheet.pdf\n",
            "Cash Flows table saved to /content/drive/My Drive/10k_filings/1850906/1850906_10k_form_cash_flows.pdf\n",
            "Statements of Operations table saved to /content/drive/My Drive/10k_filings/1850906/1850906_10k_form_statements_of_operations.pdf\n",
            "Processing 1516513_10k_form.html\n",
            "Balance Sheet table saved to /content/drive/My Drive/10k_filings/1516513/1516513_10k_form_balance_sheet.pdf\n",
            "Cash Flows table saved to /content/drive/My Drive/10k_filings/1516513/1516513_10k_form_cash_flows.pdf\n",
            "Statements of Operations table saved to /content/drive/My Drive/10k_filings/1516513/1516513_10k_form_statements_of_operations.pdf\n",
            "Processing 1851194_10k_form.html\n",
            "Balance Sheet table saved to /content/drive/My Drive/10k_filings/1851194/1851194_10k_form_balance_sheet.pdf\n",
            "Cash Flows table saved to /content/drive/My Drive/10k_filings/1851194/1851194_10k_form_cash_flows.pdf\n",
            "Statements of Operations table saved to /content/drive/My Drive/10k_filings/1851194/1851194_10k_form_statements_of_operations.pdf\n",
            "Processing 1043000_10k_form.html\n",
            "Balance Sheet table saved to /content/drive/My Drive/10k_filings/1043000/1043000_10k_form_balance_sheet.pdf\n",
            "Cash Flows table saved to /content/drive/My Drive/10k_filings/1043000/1043000_10k_form_cash_flows.pdf\n",
            "Statements of Operations table saved to /content/drive/My Drive/10k_filings/1043000/1043000_10k_form_statements_of_operations.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extracting the Tables to Text\n"
      ],
      "metadata": {
        "id": "jqrT90fDFQSN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymupdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzwAADKCKOct",
        "outputId": "c7854ab3-3752-4314-f4d0-fa8fe8ca8e46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymupdf\n",
            "  Downloading PyMuPDF-1.23.22-cp310-none-manylinux2014_x86_64.whl (4.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyMuPDFb==1.23.22 (from pymupdf)\n",
            "  Downloading PyMuPDFb-1.23.22-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (30.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.6/30.6 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDFb, pymupdf\n",
            "Successfully installed PyMuPDFb-1.23.22 pymupdf-1.23.22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "import os\n",
        "\n",
        "# Base directory containing your PDFs within subfolders\n",
        "base_directory_path = '/content/drive/My Drive/10k_filings'\n",
        "\n",
        "# Function to extract text from a PDF and save it as a text file\n",
        "def extract_text_from_pdf(pdf_path, output_directory):\n",
        "    try:\n",
        "        # Open the PDF\n",
        "        doc = fitz.open(pdf_path)\n",
        "\n",
        "        # Read text from each page and combine\n",
        "        text = \"\"\n",
        "        for page in doc:\n",
        "            text += page.get_text()\n",
        "\n",
        "        # Generate a text file path based on the PDF file name\n",
        "        text_file_path = os.path.join(output_directory, f\"{os.path.basename(pdf_path).replace('.pdf', '')}.txt\")\n",
        "\n",
        "        # Save the extracted text to a file\n",
        "        with open(text_file_path, 'w', encoding='utf-8') as text_file:\n",
        "            text_file.write(text)\n",
        "        print(f\"Extracted text saved to {text_file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while processing {pdf_path}: {e}\")\n",
        "\n",
        "# Walk through the directory structure\n",
        "for root, dirs, files in os.walk(base_directory_path):\n",
        "    for file in files:\n",
        "        if file.lower().endswith('.pdf'):\n",
        "            pdf_path = os.path.join(root, file)\n",
        "            print(f\"Processing {pdf_path}\")\n",
        "            extract_text_from_pdf(pdf_path, root)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAT2j6YiKZ9c",
        "outputId": "fe86739c-4ac1-4310-b8af-cbff0359fc8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing /content/drive/My Drive/10k_filings/1005229/1005229_10k_form_balance_sheet.pdf\n",
            "Extracted text saved to /content/drive/My Drive/10k_filings/1005229/1005229_10k_form_balance_sheet.txt\n",
            "Processing /content/drive/My Drive/10k_filings/1005229/1005229_10k_form_cash_flows.pdf\n",
            "Extracted text saved to /content/drive/My Drive/10k_filings/1005229/1005229_10k_form_cash_flows.txt\n",
            "Processing /content/drive/My Drive/10k_filings/1005229/1005229_10k_form_statements_of_operations.pdf\n",
            "Extracted text saved to /content/drive/My Drive/10k_filings/1005229/1005229_10k_form_statements_of_operations.txt\n",
            "Processing /content/drive/My Drive/10k_filings/1850906/1850906_10k_form_balance_sheet.pdf\n",
            "Extracted text saved to /content/drive/My Drive/10k_filings/1850906/1850906_10k_form_balance_sheet.txt\n",
            "Processing /content/drive/My Drive/10k_filings/1850906/1850906_10k_form_cash_flows.pdf\n",
            "Extracted text saved to /content/drive/My Drive/10k_filings/1850906/1850906_10k_form_cash_flows.txt\n",
            "Processing /content/drive/My Drive/10k_filings/1850906/1850906_10k_form_statements_of_operations.pdf\n",
            "Extracted text saved to /content/drive/My Drive/10k_filings/1850906/1850906_10k_form_statements_of_operations.txt\n",
            "Processing /content/drive/My Drive/10k_filings/1516513/1516513_10k_form_balance_sheet.pdf\n",
            "Extracted text saved to /content/drive/My Drive/10k_filings/1516513/1516513_10k_form_balance_sheet.txt\n",
            "Processing /content/drive/My Drive/10k_filings/1516513/1516513_10k_form_cash_flows.pdf\n",
            "Extracted text saved to /content/drive/My Drive/10k_filings/1516513/1516513_10k_form_cash_flows.txt\n",
            "Processing /content/drive/My Drive/10k_filings/1516513/1516513_10k_form_statements_of_operations.pdf\n",
            "Extracted text saved to /content/drive/My Drive/10k_filings/1516513/1516513_10k_form_statements_of_operations.txt\n",
            "Processing /content/drive/My Drive/10k_filings/1851194/1851194_10k_form_balance_sheet.pdf\n",
            "Extracted text saved to /content/drive/My Drive/10k_filings/1851194/1851194_10k_form_balance_sheet.txt\n",
            "Processing /content/drive/My Drive/10k_filings/1851194/1851194_10k_form_cash_flows.pdf\n",
            "Extracted text saved to /content/drive/My Drive/10k_filings/1851194/1851194_10k_form_cash_flows.txt\n",
            "Processing /content/drive/My Drive/10k_filings/1851194/1851194_10k_form_statements_of_operations.pdf\n",
            "Extracted text saved to /content/drive/My Drive/10k_filings/1851194/1851194_10k_form_statements_of_operations.txt\n",
            "Processing /content/drive/My Drive/10k_filings/1043000/1043000_10k_form_balance_sheet.pdf\n",
            "Extracted text saved to /content/drive/My Drive/10k_filings/1043000/1043000_10k_form_balance_sheet.txt\n",
            "Processing /content/drive/My Drive/10k_filings/1043000/1043000_10k_form_cash_flows.pdf\n",
            "Extracted text saved to /content/drive/My Drive/10k_filings/1043000/1043000_10k_form_cash_flows.txt\n",
            "Processing /content/drive/My Drive/10k_filings/1043000/1043000_10k_form_statements_of_operations.pdf\n",
            "Extracted text saved to /content/drive/My Drive/10k_filings/1043000/1043000_10k_form_statements_of_operations.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Combining statements to one file"
      ],
      "metadata": {
        "id": "-HWP1N7ikv2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def merge_text_files_in_directory(directory_path):\n",
        "    # Extract CIK or a unique identifier from the directory path\n",
        "    # This is a placeholder logic; you'll need to replace it with actual logic to extract the CIK\n",
        "    cik = directory_path.split(os.sep)[-1]  # Assuming the last part of the directory path is the CIK or unique ID\n",
        "\n",
        "    # Define the output file name based on the CIK\n",
        "    output_file_name = f\"{cik}_extracted_10k.txt\"\n",
        "\n",
        "    # Find all .txt files in the directory\n",
        "    text_files = [f for f in os.listdir(directory_path) if f.endswith('.txt')]\n",
        "\n",
        "    # Skip directories without text files\n",
        "    if not text_files:\n",
        "        return\n",
        "\n",
        "    # Path for the merged output file\n",
        "    output_path = os.path.join(directory_path, output_file_name)\n",
        "\n",
        "    with open(output_path, 'w', encoding='utf-8') as outfile:\n",
        "        for fname in text_files:\n",
        "            file_path = os.path.join(directory_path, fname)\n",
        "            with open(file_path, 'r', encoding='utf-8') as infile:\n",
        "                outfile.write(infile.read())\n",
        "                # Add a newline between files' content for readability\n",
        "                outfile.write(\"\\n\")\n",
        "\n",
        "    print(f\"Merged file created at {output_path}\")\n",
        "\n",
        "base_directory_path = '/content/drive/My Drive/10k_filings'\n",
        "\n",
        "# Walk through the directory structure\n",
        "for root, dirs, files in os.walk(base_directory_path):\n",
        "    merge_text_files_in_directory(root)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fgphi4NJqGFz",
        "outputId": "14996724-1ef6-483e-ed4c-f9bfc0421864"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merged file created at /content/drive/My Drive/10k_filings/1005229/1005229_extracted_10k.txt\n",
            "Merged file created at /content/drive/My Drive/10k_filings/1850906/1850906_extracted_10k.txt\n",
            "Merged file created at /content/drive/My Drive/10k_filings/1516513/1516513_extracted_10k.txt\n",
            "Merged file created at /content/drive/My Drive/10k_filings/1851194/1851194_extracted_10k.txt\n",
            "Merged file created at /content/drive/My Drive/10k_filings/1043000/1043000_extracted_10k.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataframe creation"
      ],
      "metadata": {
        "id": "Eac2DnMkspCi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def create_dataframe_from_files(directory_path):\n",
        "    data = []  # List to store the file names and their text contents\n",
        "\n",
        "    # Walk through the directory and subdirectories\n",
        "    for root, dirs, files in os.walk(directory_path):\n",
        "        for file in files:\n",
        "            # Check if the file matches the pattern {cik}_extracted_10k.txt\n",
        "            if file.endswith('_extracted_10k.txt'):\n",
        "                file_path = os.path.join(root, file)\n",
        "                # Open and read the content of the file\n",
        "                with open(file_path, 'r', encoding='utf-8') as file_content:\n",
        "                    text = file_content.read()\n",
        "                    # Append the filename (without extension) and text to the list\n",
        "                    data.append({'filename': os.path.splitext(file)[0], 'text': text})\n",
        "\n",
        "    # Create a DataFrame from the list\n",
        "    df = pd.DataFrame(data, columns=['filename', 'text'])\n",
        "    return df\n",
        "\n",
        "base_directory_path = '/content/drive/My Drive/10k_filings'\n",
        "df = create_dataframe_from_files(base_directory_path)\n",
        "\n",
        "# Display the DataFrame to verify\n",
        "print(df.head())\n",
        "\n",
        "# Optional: Save the DataFrame to a CSV file\n",
        "df.to_csv('/content/drive/My Drive/10k_filings/extracted_10k_dataframe.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PoGkDsJgsnkY",
        "outputId": "bf7ddfd0-0f75-4111-96f9-02b4edb88bdb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                filename                                               text\n",
            "0  1005229_extracted_10k   \\nMarch 31,\\n \\n2023\\n2022\\n \\n(In thousands,...\n",
            "1  1850906_extracted_10k   \\n \\nDecember 31,\\n \\n \\n \\n2022\\n  \\n2021\\n ...\n",
            "2  1516513_extracted_10k  As of March 31,\\n2023\\n2022\\nAssets\\nCurrent a...\n",
            "3  1851194_extracted_10k   \\n \\nDecember 31,\\n \\n \\n \\n2022\\n  \\n2021\\n ...\n",
            "4  1043000_extracted_10k   \\nDecember 31,\\n \\n2022\\n2021\\nASSETS\\nCurren...\n"
          ]
        }
      ]
    }
  ]
}