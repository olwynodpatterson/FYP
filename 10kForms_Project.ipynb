{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1tiP0FKqlZHu3YcKGY435RxgJ6ScDE1Ev",
      "authorship_tag": "ABX9TyOS4SMPT6DNPdr5xkK9J+mT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olwynodpatterson/FYP/blob/main/10kForms_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "SNoAMHMgyLLx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cd3742c-a91e-4fc7-bf31-d26e1fcf89a6"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scraping 10k fillings"
      ],
      "metadata": {
        "id": "0ALLdZ_9aNvH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "# Function to retrieve a list of tickers from a given URL\n",
        "def get_ticker_list(url):\n",
        "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=headers)\n",
        "    # Check if the request was successful\n",
        "    if response.status_code == 200:\n",
        "        tickers = response.text.splitlines()\n",
        "        return tickers\n",
        "    else:\n",
        "        # Raise an exception if the request failed\n",
        "        raise Exception(f\"Failed to retrieve data. Status code: {response.status_code}\")\n",
        "\n",
        "# Function to choose a random CIK (Central Index Key) from the list of tickers\n",
        "def choose_random_cik(tickers):\n",
        "    # Choose a random ticker and associated CIK\n",
        "    ticker, cik = random.choice(tickers).split('\\t')\n",
        "    return cik\n",
        "\n",
        "# Function to get the URLs of 10-K filings for a given CIK\n",
        "def get_10k_filing_urls(cik):\n",
        "    time.sleep(3) # Delay to avoid overwhelming the server\n",
        "    # Fetch the URLs for 10-K filings\n",
        "    filings_url = f\"https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK={cik}&type=10-K&dateb=&owner=exclude&count=10\"\n",
        "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(filings_url, headers=headers)\n",
        "    # Check response status\n",
        "    if response.status_code != 200:\n",
        "        print(\"Error fetching filings:\", response.status_code)\n",
        "        return None\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    table = soup.find('table', class_='tableFile2')\n",
        "    if not table:\n",
        "        return None\n",
        "\n",
        "    # Parse the table to get the filing links\n",
        "    for row in table.find_all('tr')[1:]:\n",
        "        cols = row.find_all('td')\n",
        "        if len(cols) > 1:\n",
        "            documents_page_link = 'https://www.sec.gov' + cols[1].a['href']\n",
        "            submission_text_file_link = documents_page_link.replace('-index.htm', '.txt')\n",
        "            return submission_text_file_link  # Return the first link found\n",
        "\n",
        "    return None\n",
        "\n",
        "def dehtml(html_content):\n",
        "    # Use BeautifulSoup to parse the HTML content\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "    # Extract text from the parsed HTML\n",
        "    text = soup.get_text()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def download_file(url, cik, folder='/content/drive/My Drive/10k_filings'):\n",
        "    # Define the Google Drive folder path for the specific CIK\n",
        "    drive_folder_path = os.path.join(folder, cik)\n",
        "\n",
        "    # Create the folder if it doesn't exist\n",
        "    if not os.path.exists(drive_folder_path):\n",
        "        os.makedirs(drive_folder_path)\n",
        "\n",
        "    # Correct the file paths to include the CIK-specific folder\n",
        "    html_file_path = os.path.join(drive_folder_path, f'{cik}_10k_form.html')\n",
        "    text_file_path = os.path.join(drive_folder_path, f'{cik}_10k_form.txt')\n",
        "\n",
        "    # Make a request to download the file\n",
        "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=headers, stream=True)\n",
        "    if response.status_code == 200:\n",
        "        with open(html_file_path, 'wb') as html_file:\n",
        "            for chunk in response.iter_content(chunk_size=8192):\n",
        "                html_file.write(chunk)\n",
        "        print(f\"Downloaded HTML: {html_file_path}\")\n",
        "\n",
        "        # Read the HTML content\n",
        "        with open(html_file_path, 'r', encoding='utf-8') as html_file:\n",
        "            html_content = html_file.read()\n",
        "\n",
        "        # Convert HTML to plain text\n",
        "        plain_text = dehtml(html_content)\n",
        "\n",
        "        # Save the plain text\n",
        "        with open(text_file_path, 'w', encoding='utf-8') as text_file:\n",
        "            text_file.write(plain_text)\n",
        "        print(f\"Converted to Text: {text_file_path}\")\n",
        "    else:\n",
        "        print(f\"Failed to download file. Status code: {response.status_code}\")\n",
        "\n",
        "\n",
        "# Main function to drive the program\n",
        "def main():\n",
        "    url = 'https://www.sec.gov/include/ticker.txt'\n",
        "    tickers = get_ticker_list(url)\n",
        "    i=0\n",
        "    while i <1:\n",
        "        cik =  choose_random_cik(tickers)\n",
        "        print(f\"Selected CIK: {cik}\")\n",
        "        filing_url = get_10k_filing_urls(cik)\n",
        "        print(f\"Filing URL: {filing_url}\")\n",
        "        if filing_url:\n",
        "            download_file(filing_url, cik)\n",
        "            time.sleep(1)  # Respectful delay between requests\n",
        "            i += 1\n",
        "        else:\n",
        "            print(f\"No 10-K filings found for CIK {cik}.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXp4d8ZyppGd",
        "outputId": "bb4736ff-e85f-40e2-ae4a-b384b1788d2c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Selected CIK: 1851174\n",
            "Filing URL: https://www.sec.gov/Archives/edgar/data/1851174/000156459022012951/0001564590-22-012951.txt\n",
            "Downloaded HTML: /content/drive/My Drive/10k_filings/1851174/1851174_10k_form.html\n",
            "Converted to Text: /content/drive/My Drive/10k_filings/1851174/1851174_10k_form.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finding the Balance Sheet"
      ],
      "metadata": {
        "id": "WiVZvR9mYyi0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Directory containing your 10-K filings\n",
        "base_directory_path = '/content/drive/My Drive/10k_filings'\n",
        "\n",
        "# Loop through all subdirectories in the base directory\n",
        "for subdir in os.listdir(base_directory_path):\n",
        "    subdir_path = os.path.join(base_directory_path, subdir)\n",
        "\n",
        "    # Check if the path is a directory to avoid trying to open files directly\n",
        "    if os.path.isdir(subdir_path):\n",
        "        # Loop through all files in the subdirectory\n",
        "        for filename in os.listdir(subdir_path):\n",
        "            # Check if the file is an HTML file\n",
        "            if filename.endswith('.html'):\n",
        "                print(filename)\n",
        "                file_path = os.path.join(subdir_path, filename)\n",
        "\n",
        "                # Open and read the file\n",
        "                with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                    html_content = file.read()\n",
        "\n",
        "                # Parse the HTML content\n",
        "                soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "                # Find the <a> tag that contains 'Consolidated Balance Sheet'\n",
        "                a_tag = soup.find('a', string=lambda text: text and 'Balance Sheet' in text)\n",
        "                #print(f\"Found a tag {a_tag}\")\n",
        "                if a_tag and a_tag.has_attr('href'):\n",
        "                    href = a_tag['href']\n",
        "\n",
        "                    # Check if it's an internal link\n",
        "                    if href.startswith('#'):\n",
        "                        target_id = href[1:]  # Remove the '#' at the beginning\n",
        "                        target_element = soup.find(id=target_id)\n",
        "\n",
        "                        if target_element:\n",
        "                            print(f\"Found the target element in {filename}\")\n",
        "\n",
        "                            # Find the next table after the target element\n",
        "                            next_table = target_element.find_next('table')\n",
        "                            if next_table:\n",
        "                                print(f\"Found the next table in {filename}\")\n",
        "                                html_string = str(next_table)\n",
        "\n",
        "                                # Use Pandas to read the HTML string; this returns a list of DataFrames\n",
        "                                dfs = pd.read_html(html_string, header=1)\n",
        "\n",
        "                                # Assuming there is only one table\n",
        "                                table_df = dfs[0]\n",
        "\n",
        "                                # Generate a dynamic CSV file path based on the HTML file name\n",
        "                                csv_file_path = os.path.join(subdir_path, filename.replace('.html', '_balance_sheet.csv'))\n",
        "\n",
        "                                # Export the DataFrame to a CSV file\n",
        "                                table_df.to_csv(csv_file_path, index=False)\n",
        "                                print(f\"Table saved to {csv_file_path}\")\n",
        "\n",
        "                            else:\n",
        "                                print(f\"No next table found in {filename}.\")\n",
        "\n",
        "                        else:\n",
        "                            print(f\"Target element not found in {filename}.\")\n",
        "                    else:\n",
        "                        print(f\"The link in {filename} is not an internal link.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3qEW1EharRP",
        "outputId": "9bad7915-9096-4809-a9d2-70972e9c8726"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1464423_10k_form.html\n",
            "Found the target element in 1464423_10k_form.html\n",
            "Found the next table in 1464423_10k_form.html\n",
            "Table saved to /content/drive/My Drive/10k_filings/1464423/1464423_10k_form_balance_sheet.csv\n",
            "1005229_10k_form.html\n",
            "Found the target element in 1005229_10k_form.html\n",
            "Found the next table in 1005229_10k_form.html\n",
            "Table saved to /content/drive/My Drive/10k_filings/1005229/1005229_10k_form_balance_sheet.csv\n",
            "1495932_10k_form.html\n",
            "Found the target element in 1495932_10k_form.html\n",
            "Found the next table in 1495932_10k_form.html\n",
            "Table saved to /content/drive/My Drive/10k_filings/1495932/1495932_10k_form_balance_sheet.csv\n",
            "1815753_10k_form.html\n",
            "Found the target element in 1815753_10k_form.html\n",
            "Found the next table in 1815753_10k_form.html\n",
            "Table saved to /content/drive/My Drive/10k_filings/1815753/1815753_10k_form_balance_sheet.csv\n",
            "1851174_10k_form.html\n",
            "Found the target element in 1851174_10k_form.html\n",
            "Found the next table in 1851174_10k_form.html\n",
            "Table saved to /content/drive/My Drive/10k_filings/1851174/1851174_10k_form_balance_sheet.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cleaning the Data"
      ],
      "metadata": {
        "id": "ktgKf32wFMfa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def clean_columns_based_on_criteria(df):\n",
        "    while len(df.columns) > 3:\n",
        "        i = 0  # Start with the first column\n",
        "        while i < len(df.columns) - 1:  # Ensure there is a next column to compare\n",
        "            # Compare adjacent columns (ignoring the first row)\n",
        "            if df.iloc[1:, i].equals(df.iloc[1:, i+1]):\n",
        "                df.drop(df.columns[i], axis=1, inplace=True)  # Drop the first of the duplicates\n",
        "            else:\n",
        "                # Count nulls ignoring the first row\n",
        "                nulls_first_col = df.iloc[1:, i].isnull().sum()\n",
        "                nulls_second_col = df.iloc[1:, i+1].isnull().sum()\n",
        "                # Drop the column with more nulls\n",
        "                if nulls_first_col > nulls_second_col:\n",
        "                    df.drop(df.columns[i], axis=1, inplace=True)\n",
        "                    i -= 1  # Adjust index if a column is removed\n",
        "                else:\n",
        "                    df.drop(df.columns[i+1], axis=1, inplace=True)\n",
        "            i += 1\n",
        "    return df\n",
        "\n",
        "def clean_csv_file(input_path, output_path):\n",
        "    # Load the DataFrame from the CSV file\n",
        "    df = pd.read_csv(input_path)\n",
        "\n",
        "    # Apply the cleaning process\n",
        "    cleaned_df = clean_columns_based_on_criteria(df)\n",
        "\n",
        "    # Save the cleaned DataFrame to a new CSV file\n",
        "    cleaned_df.to_csv(output_path, index=False)\n",
        "    print(f\"Cleaned file saved at {output_path}\")\n",
        "\n",
        "# Base directory containing your 10-K filings\n",
        "base_input_directory = '/content/drive/My Drive/10k_filings'\n",
        "\n",
        "# Iterate through all subdirectories in the base input directory\n",
        "for subdir in os.listdir(base_input_directory):\n",
        "    subdir_path = os.path.join(base_input_directory, subdir)\n",
        "    if os.path.isdir(subdir_path):  # Ensure it's a directory\n",
        "        # Process each CSV file within the subdirectory\n",
        "        for filename in os.listdir(subdir_path):\n",
        "            if filename.endswith('.csv'):\n",
        "                input_path = os.path.join(subdir_path, filename)\n",
        "                # Output file path remains within the same subdirectory\n",
        "                output_path = os.path.join(subdir_path, filename.replace('.csv', '_cleaned.csv'))\n",
        "                clean_csv_file(input_path, output_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9a90cm38DNzI",
        "outputId": "03993547-1da8-4867-a2e2-75206ed74800"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned file saved at /content/drive/My Drive/10k_filings/1464423/1464423_10k_form_balance_sheet_cleaned.csv\n",
            "Cleaned file saved at /content/drive/My Drive/10k_filings/1005229/1005229_10k_form_balance_sheet_cleaned.csv\n",
            "Cleaned file saved at /content/drive/My Drive/10k_filings/1495932/1495932_10k_form_balance_sheet_cleaned.csv\n",
            "Cleaned file saved at /content/drive/My Drive/10k_filings/1815753/1815753_10k_form_balance_sheet_cleaned.csv\n",
            "Cleaned file saved at /content/drive/My Drive/10k_filings/1851174/1851174_10k_form_balance_sheet_cleaned.csv\n"
          ]
        }
      ]
    }
  ]
}